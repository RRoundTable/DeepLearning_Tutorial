{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recurrent Neural Network\n",
    "\n",
    "- feedforward \n",
    "- backpropation\n",
    "- sequence 계열 데이터에 사용되는 모델\n",
    "\n",
    "### data\n",
    "\n",
    "### reference : cs231n RNN 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "# reference : https://github.com/aisolab/CS20\n",
    "sentences = [['I', 'feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "# 형태소\n",
    "pos = [['pronoun', 'verb', 'adjective'],\n",
    "     ['noun', 'verb', 'adverb', 'adjective'],\n",
    "     ['noun', 'verb', 'determiner', 'noun', 'preposition', 'adjective', 'noun'],\n",
    "     ['noun', 'verb', 'adverb', 'adjective', 'verb']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word dict\n",
    "word_list = []\n",
    "for elm in sentences:\n",
    "    word_list += elm\n",
    "word_list = list(set(word_list)) # unique한 word만 list\n",
    "word_list.sort()\n",
    "word_list = ['<pad>'] + word_list # '<pad>' 추가\n",
    "word_to_ix = {word : idx for idx, word in enumerate(word_list)}  # word\n",
    "ix_to_word={idx: word for idx, word in enumerate(word_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'I': 1, 'a': 2, 'changing': 3, 'deep': 4, 'difficult': 5, 'fast': 6, 'feel': 7, 'for': 8, 'framework': 9, 'hungry': 10, 'is': 11, 'learning': 12, 'tensorflow': 13, 'very': 14}\n",
      "{0: '<pad>', 1: 'I', 2: 'a', 3: 'changing', 4: 'deep', 5: 'difficult', 6: 'fast', 7: 'feel', 8: 'for', 9: 'framework', 10: 'hungry', 11: 'is', 12: 'learning', 13: 'tensorflow', 14: 'very'}\n"
     ]
    }
   ],
   "source": [
    "# 확인하기\n",
    "print(word_to_ix)\n",
    "print(ix_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size\n",
    "data_size, vocab_size=len([ word for i in sentences for word in i]), len(word_to_ix)\n",
    "data=[ word for i in sentences for word in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "hidden_size=100 # size of hidden layer of neurons\n",
    "seq_length=10 # number of steps to unroll the RNN\n",
    "learning_rate=1e-1\n",
    "\n",
    "# model parameter\n",
    "Wxh = np.random.randn(hidden_size,vocab_size)*0.01 # input to hidden\n",
    "Whh= np.random.randn(hidden_size,hidden_size)*0.01 # hidden to hidden\n",
    "Why= np.random.randn(vocab_size,hidden_size)*0.01 # hiddent to output\n",
    "bh =np.zeros((hidden_size,1))\n",
    "by = np.zeros((vocab_size,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling\n",
    "def sample(h,seed_ix,n):\n",
    "    \"\"\"\n",
    "    sample a sequence of integers from the model\n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    return sample_ix\n",
    "    \"\"\"\n",
    "    \n",
    "    x=np.zeros((vocab_size,1)) # 가능한 정답지 vocab\n",
    "    x[seed_ix]=1 # seed_ix에 해당하는 index만 1 나머지는 모두 0\n",
    "    ixes=[]\n",
    "    \n",
    "    \"\"\" \n",
    "    xrange 타입은 수정이 불가한 순차적 접근 가능한 데이터 타입이다. \n",
    "    xrange 타입의 장점이라고 하면 지정한 데이터 크기에 상관없이 memory 할당량이 일정하다는 것이다.\n",
    "    \"\"\"\n",
    "    for t in range(n):\n",
    "        h=np.tanh(np.dot(Wxh,x)+np.dot(Whh,h)+bh) # next hidden state\n",
    "        y=np.dot(Why,h)+by # y값 예측\n",
    "        p=np.exp(y)/np.sum(np.exp(y)) # print(p) 해보기\n",
    "        # np.random.choice : p를 지정하면 각 p에 속하는 확률로 sampling\n",
    "        ix=np.random.choice(range(vocab_size),p=p.ravel()) # ravel : 다차원 배열을 1차원 배열로 평평하게 변환\n",
    "        x=np.zeros((vocab_size,1))\n",
    "        x[ix]=1\n",
    "        ixes.append(ix)\n",
    "        \n",
    "    return ixes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function : forward pass + backward pass\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    input : list of integer\n",
    "    target : list of interger\n",
    "    hprev :  Hx1 array of initial state\n",
    "    return the loss, gradient on model parameters, and last hiddent state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps={},{},{},{}\n",
    "    hs[-1]=np.copy(hprev)\n",
    "    loss=0\n",
    "    \n",
    "    # forward pass\n",
    "    print(len(inputs))\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t]=np.zeros((vocab_size,1)) # encode one-hot\n",
    "        xs[t][inputs[t]]=1\n",
    "        hs[t]=np.tanh(np.dot(Wxh,xs[t])+np.dot(Whh,hs[t-1])+bh) # hidden state\n",
    "        ys[t]=np.dot(Why,hs[t])+by # unnormalized log probabilities for next chars\n",
    "        ps[t]=np.exp(ys[t])/np.sum(np.exp(ys[t])) # probabilites for next chars\n",
    "        print(t) #18까지는 되는데\n",
    "        loss+=-np.log(ps[t][targets[t],0]) # softmax (cross-entropy)\n",
    "    # backward pass\n",
    "    \n",
    "    dWxh, dWhh, dWhy=np.zeros_like(Wxh),np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np. zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext=np.zeros_like(hs[0])\n",
    "    \n",
    "    for t in reversed(range(len(inputs))): # 역순으로 진행한다\n",
    "        dy=np.copy(ps[t])\n",
    "        dy[targets[t]]-=1 # backpropagation into y\n",
    "        dWhy+=np.dot(dy,hs[t].T)\n",
    "        dby+=dy\n",
    "        dh=np.dot(Why.T,dy)+dhnext\n",
    "        dhraw=(1-hs[t]*hs[t])*dh\n",
    "        dbh+=dhraw\n",
    "        dWxh+=np.dot(dhraw,xs[t].T)\n",
    "        dWhh+=np.dot(dhraw,hs[t-1].T)\n",
    "        dhnext=np.dot(Whh.T,dhraw)\n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "    return loss, dWxh, dWhh, dWhh, dbh,dby, hs[len(inputs)-1]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "----------\n",
      " deep <pad> difficult fast a very changing changing a feel learning changing for I for deep feel is I is fast tensorflow for hungry fast tensorflow feel a very very deep learning for hungry <pad> framework very difficult <pad> changing fast difficult very framework I difficult <pad> hungry difficult deep hungry <pad> for <pad> a learning changing a fast learning I changing feel deep tensorflow fast very feel deep for learning I feel fast <pad> for is is is fast framework deep hungry learning fast deep changing feel is fast difficult framework very is changing tensorflow difficult difficult difficult changing framework very learning a feel difficult feel for <pad> <pad> hungry very hungry changing deep hungry changing for very feel a <pad> feel fast learning deep fast hungry fast deep very fast framework a deep is hungry is deep I difficult hungry I learning hungry is feel difficult for is deep deep tensorflow for hungry tensorflow learning framework <pad> very deep <pad> tensorflow is changing I I learning feel very difficult hungry is difficult deep deep difficult fast <pad> for a a I I tensorflow feel framework deep for for tensorflow a difficult a tensorflow fast for framework tensorflow framework \n",
      " ---------------\n",
      "10\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "iter 0, loss: 67.189165\n",
      "(100, 100)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (15,100) (100,100) (15,100) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-9d8518f0b6af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mWhh\u001b[0m\u001b[1;33m-=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdWhh\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdWhy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mWhy\u001b[0m\u001b[1;33m-=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdWhy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mbh\u001b[0m\u001b[1;33m-=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdbh\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mby\u001b[0m\u001b[1;33m-=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdby\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (15,100) (100,100) (15,100) "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # input 준비하기\n",
    "    if p+seq_length+1>=len(data) or n==0:\n",
    "        hprev=np.zeros((hidden_size,1)) # Reset RNN hidden state\n",
    "        p=0 # go from start of data\n",
    "    inputs=[word_to_ix[word] for word in data[p:p+seq_length]]\n",
    "    targets=[word_to_ix[word] for word in data[p+1:p+seq_length+1]]\n",
    "    print(len(data))\n",
    "    \n",
    "    # sample from the model now and then\n",
    "    if n%100==0:\n",
    "        \n",
    "        sample_ix=sample(hprev,inputs[0],200)\n",
    "        txt=\" \".join(ix_to_word[ix] for ix in sample_ix)\n",
    "        print('----------\\n %s \\n ---------------'%txt)\n",
    "        \n",
    "    # forward pass\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 100 == 0: print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "        \n",
    "    # parameter updata\n",
    "    # perform parameter update with Adagrad\n",
    "    \n",
    "    Wxh-=learning_rate*dWxh\n",
    "    Whh-=learning_rate*dWhh\n",
    "    print(dWhy.shape) # 100,100 에러가 생기는 부분\n",
    "    Why-=learning_rate*dWhy\n",
    "    bh-=learning_rate*dbh\n",
    "    by-=learning_rate*dby\n",
    "#     for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "#                                 [dWxh, dWhh, dWhy, dbh, dby], \n",
    "#                                 [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "# #         mem += dparam* dparam\n",
    "# #         param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "# #         #print(dparam.shape)\n",
    "# #         #param-=learning_rate*dparam\n",
    "\n",
    "\n",
    "    p+=seq_length\n",
    "    n+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
